{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Client Assessment Report Tool","text":"<p>The purpose of this project is to evaluate how well open source large language models (LLM) would work in assisting the generation of text for a client assessment report.  The generated  text will be reviewed by a user and allow them to correct errors and suggest additional  improvements.  </p> <p>This project will be configured to only use locally running LLMs to ensure the confidential client  data is not exposed to other vendors.</p>"},{"location":"#project-layout","title":"Project Layout","text":"<pre><code>.github/       # Project Github actions and workflows.\ndata/          # Project data files used for parsing reports.\ndocs/          # Project documentation files.\nnotebooks/     # Project Jupyter notebook files.\nscripts/       # Project script files.\nsource/        # Python source files.\ntests/         # Python test source files.\n\n.env           # Environment variables file.\nconfig.toml    # Project configuration file.\nMakefile       # Project makefile containing developer commands.\nmkdocs.yml     # Project documentation configuration file.\npoetry.lock    # Poetry python dependencies files.\npoetry.toml    # Poetry configuration file.\npyproject.toml # Python project file.\n</code></pre>"},{"location":"#project-source-code","title":"Project Source Code","text":"<p>The source code for this project is available at Github.</p>"},{"location":"design/","title":"Design","text":""},{"location":"design/#report-parsing","title":"Report Parsing","text":"<p>The client assessment reports are saved in the <code>.docx</code> file format.  The document contains multiple sections.  Each section contains common descriptive text and client specific text.  Only the client specific text needs to be extracted for this project.</p> <p>The report also contains sections with data tables that contain specific test results. The code has been written to extract this client tabular data but it is currently not being used by the project.</p>"},{"location":"design/#vector-database","title":"Vector Database","text":"<p>After the client specific information has been extracted from the report, it will be persisted into a local vector database.  The project is currently using the  LanceDB vector database.  This local database supports both vector embedding simularity searching along with full text search.</p> <p>The client data is saved into two different tables.  These tables provide a parent-child type relationship.  The initial strategy is to find all the existing clients with similar keywords.  The text from each of these client report sections will then be used as contextual input for the generation of text for the new client.</p> <p>The database table schemas are defined using PyDantic model classes which are defined in the following source code file.</p> <pre><code>source/llmtool/data/models.py\n</code></pre>"},{"location":"design/#document-table","title":"Document Table","text":"<p>The Document table holds the client information along with the keywords used to  describe the client.  The keyword text values are saved along with a vector embedding of the keywords.  This allows this table to support both simularity/vector and full text searches.  </p>"},{"location":"design/#section-table","title":"Section Table","text":"<p>The Section table holds the extracted text from each of the sections that were defined in the parsed client assessment report.  The sections extracted text is also embedded as a  vector into the table but this is not currently being used.</p>"},{"location":"design/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":""},{"location":"design/#single-prompt-text-generation","title":"Single Prompt Text Generation","text":""},{"location":"design/#multi-agent-text-generation","title":"Multi-Agent Text Generation","text":""},{"location":"design/#agents","title":"Agents","text":"<ul> <li>Reasons Writer</li> <li>Section Writer</li> <li>Editor</li> </ul>"},{"location":"design/#tools","title":"Tools","text":"<ul> <li>Query LanceDB for similar clients</li> <li>Query LanceDB for specific section text and filtering by specified client list</li> </ul>"},{"location":"design/#tasks","title":"Tasks","text":"<ul> <li>Get similar clients to the current client</li> <li>Generate key reason text for client</li> <li>Generate section text for client</li> <li>Correct any errors or mistakes in generated text</li> <li></li> </ul>"},{"location":"development/","title":"Development Environment","text":"<p>This is a Python based project (version 3.10) and the development environment can be setup using the following directions.  Please review the Makefile that is in the  project root directory.  It contains many commands for executing the different development  tools used by this project.</p>"},{"location":"development/#python","title":"Python","text":"<ol> <li>Install the Python Package Manager Poetry.</li> <li>Clone repository source code into a local directory.</li> <li>Install required packages by running command <code>poetry install</code>.</li> </ol>"},{"location":"development/#ollama","title":"Ollama","text":"<p>This project was designed to use the Large Language Model (LLM) tool Ollama for executing the different open source models locally.  This tool must be installed and configured for the application to work properly.  See the Ollama website for more information  and installation instructions.</p>"},{"location":"development/#models","title":"Models","text":"<p>The ollama tool must be used to download any model that is going to be used by the project. The following command is used to download a model so it can be run locally.</p> <pre><code>ollama pull &lt;model-name&gt; # pulls down the model\nollama list              # lists which models are locally available\n</code></pre>"},{"location":"development/#custom-models","title":"Custom Models","text":"<p>This project is using the CrewAI framework to run multiple AI Agents to see if they can improve the quality of the generated text.  This framework may require the open source LLM model files to be updated.  The <code>scripts/ollama</code> folder contains some updated  model files and bash scripts to update the local models to work with CrewAI.  The Ollama Integration documentation on the CrewAI website decribes this procedure in more detail.</p> <pre><code>cd scripts/ollama             # changes to the ollama scripts directory\nbash create-llama2-model.sh   # creates new model based on llama2\nbash create-llama3-model.sh   # creates new model based on llama3\nbash create-phi3.sh           # creates new model based on phi3\n</code></pre>"},{"location":"development/datafiles/","title":"Assessment Report Data Files","text":"<p>The source code repository does not contain any examples of client assessment reports since they contain confidential information.  There are some Jupyter Notebook script  files located in the <code>notebook</code> folder that were used to generate the following data files.</p> <p>These data files are used to parse and extract the required text from the client assessment  report documents.</p> <pre><code>config/template_sections.json   # Defines report sections to be parsed.\nconfig/template_tables.json     # Defines report tables to be parsed.\nconfig/template_keywords.csv    # Defines keywords used for simlarity search.\n</code></pre>"},{"location":"development/documents/","title":"Project Documentation","text":"<p>The tool MkDocs is used to build a static website from all the project documentation markdown files located in the <code>docs</code> folder. The configuration file <code>mkdocs.yml</code> is located in the project root directory.</p> <p>Below are the Makefile commands used to build the project documentation.</p> <pre><code>make docs        # Build and serve the project documentation.\nmake docs-test   # Build project documentation and check for errors.\n</code></pre>"},{"location":"development/environment/","title":"Environment Variables","text":"<p>Use the following environment variables to allow the project software to connect to the local models running inside ollama.</p> <pre><code>OPENAI_API_BASE='http://localhost:11434/v1'\nOPENAI_MODEL_NAME='llama3:8b'\nOPENAI_API_KEY=''\n</code></pre> <p>The project has been configured to support the use of a <code>.env</code> file to hold all the required environment variables.  This file is included in the <code>.gitignore</code> file and will not be checked into the Git repository. It can be loaded into the local active shell by executing the following command.</p> <pre><code>source scripts/setup.sh\n</code></pre>"},{"location":"workflow/","title":"Workflow","text":"<p>The typical development workflow is list below.</p> <pre><code>...                    # download the client assessment reports\nmake install           # ensure project CLI has been installed\nvi config.toml         # update the project configuration values\n\nllmtool tools load     # parse and save reports into local database\nllmtool tools search   # query local database (optional)\nllmtool gui run        # run the web based GUI\n</code></pre>"},{"location":"workflow/#configuration-file","title":"Configuration File","text":""},{"location":"workflow/cli/","title":"CLI (Command Line Interface)","text":""}]}